{"version":3,"file":"chat_models.cjs","names":["messages: BaseMessage[]","messagesToTrace: BaseMessage[]","isURLContentBlock","isBase64ContentBlock","convertToOpenAIImageBlock","BaseLanguageModel","fields: BaseChatModelParams","iife","getEnvironmentVariable","options?: Partial<CallOptions>","input: BaseLanguageModelInput","options?: CallOptions","_messages: BaseMessage[]","_options: this[\"ParsedCallOptions\"]","_runManager?: CallbackManagerForLLMRun","CallbackManager","generationChunk: ChatGenerationChunk | undefined","llmOutput: Record<string, any> | undefined","castStandardMessageContent","isAIMessageChunk","options: this[\"ParsedCallOptions\"]","messages: BaseMessageLike[][]","parsedOptions: this[\"ParsedCallOptions\"]","handledOptions: RunnableConfig","startedRunManagers?: CallbackManagerForLLMRun[]","coerceMessageLikeToMessage","runManagers: CallbackManagerForLLMRun[] | undefined","generations: ChatGeneration[][]","llmOutputs: LLMResult[\"llmOutput\"][]","callbackHandlerPrefersStreaming","concat","output: LLMResult","RUN_KEY","missingPromptIndices: number[]","generations: Generation[][]","result","isBaseMessage","isAIMessage","options?: string[] | CallOptions","callbacks?: Callbacks","parsedOptions: CallOptions | undefined","_options?: this[\"ParsedCallOptions\"]","promptValues: BasePromptValueInterface[]","promptMessages: BaseMessage[][]","outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>","config?: StructuredOutputMethodOptions<boolean>","schema: Record<string, any> | InteropZodType<RunOutput>","getSchemaDescription","tools: ToolDefinition[]","isInteropZodSchema","toJsonSchema","RunnableLambda","input: BaseMessageChunk","AIMessageChunk","RunnablePassthrough","input: any","config","RunnableSequence","runManager?: CallbackManagerForLLMRun","AIMessage"],"sources":["../../src/language_models/chat_models.ts"],"sourcesContent":["import type { ZodType as ZodTypeV3 } from \"zod/v3\";\nimport type { $ZodType as ZodTypeV4 } from \"zod/v4/core\";\nimport {\n  AIMessage,\n  type BaseMessage,\n  BaseMessageChunk,\n  type BaseMessageLike,\n  coerceMessageLikeToMessage,\n  AIMessageChunk,\n  isAIMessageChunk,\n  isBaseMessage,\n  isAIMessage,\n  MessageOutputVersion,\n} from \"../messages/index.js\";\nimport {\n  convertToOpenAIImageBlock,\n  isURLContentBlock,\n  isBase64ContentBlock,\n} from \"../messages/content/data.js\";\nimport type { BasePromptValueInterface } from \"../prompt_values.js\";\nimport {\n  LLMResult,\n  RUN_KEY,\n  type ChatGeneration,\n  ChatGenerationChunk,\n  type ChatResult,\n  type Generation,\n} from \"../outputs.js\";\nimport {\n  BaseLanguageModel,\n  type StructuredOutputMethodOptions,\n  type ToolDefinition,\n  type BaseLanguageModelCallOptions,\n  type BaseLanguageModelInput,\n  type BaseLanguageModelParams,\n} from \"./base.js\";\nimport {\n  CallbackManager,\n  type CallbackManagerForLLMRun,\n  type Callbacks,\n} from \"../callbacks/manager.js\";\nimport type { RunnableConfig } from \"../runnables/config.js\";\nimport type { BaseCache } from \"../caches/index.js\";\nimport {\n  StructuredToolInterface,\n  StructuredToolParams,\n} from \"../tools/index.js\";\nimport {\n  Runnable,\n  RunnableLambda,\n  RunnableSequence,\n  RunnableToolLike,\n} from \"../runnables/base.js\";\nimport { concat } from \"../utils/stream.js\";\nimport { RunnablePassthrough } from \"../runnables/passthrough.js\";\nimport {\n  getSchemaDescription,\n  InteropZodType,\n  isInteropZodSchema,\n} from \"../utils/types/zod.js\";\nimport { callbackHandlerPrefersStreaming } from \"../callbacks/base.js\";\nimport { toJsonSchema } from \"../utils/json_schema.js\";\nimport { getEnvironmentVariable } from \"../utils/env.js\";\nimport { castStandardMessageContent, iife } from \"./utils.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type ToolChoice = string | Record<string, any> | \"auto\" | \"any\";\n\n/**\n * Represents a serialized chat model.\n */\nexport type SerializedChatModel = {\n  _model: string;\n  _type: string;\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\n// todo?\n/**\n * Represents a serialized large language model.\n */\nexport type SerializedLLM = {\n  _model: string;\n  _type: string;\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\n/**\n * Represents the parameters for a base chat model.\n */\nexport type BaseChatModelParams = BaseLanguageModelParams & {\n  /**\n   * Whether to disable streaming.\n   *\n   * If streaming is bypassed, then `stream()` will defer to\n   * `invoke()`.\n   *\n   * - If true, will always bypass streaming case.\n   * - If false (default), will always use streaming case if available.\n   */\n  disableStreaming?: boolean;\n  /**\n   * Version of `AIMessage` output format to store in message content.\n   *\n   * `AIMessage.contentBlocks` will lazily parse the contents of `content` into a\n   * standard format. This flag can be used to additionally store the standard format\n   * as the message content, e.g., for serialization purposes.\n   *\n   * - \"v0\": provider-specific format in content (can lazily parse with `.contentBlocks`)\n   * - \"v1\": standardized format in content (consistent with `.contentBlocks`)\n   *\n   * You can also set `LC_OUTPUT_VERSION` as an environment variable to \"v1\" to\n   * enable this by default.\n   *\n   * @default \"v0\"\n   */\n  outputVersion?: MessageOutputVersion;\n};\n\n/**\n * Represents the call options for a base chat model.\n */\nexport type BaseChatModelCallOptions = BaseLanguageModelCallOptions & {\n  /**\n   * Specifies how the chat model should use tools.\n   * @default undefined\n   *\n   * Possible values:\n   * - \"auto\": The model may choose to use any of the provided tools, or none.\n   * - \"any\": The model must use one of the provided tools.\n   * - \"none\": The model must not use any tools.\n   * - A string (not \"auto\", \"any\", or \"none\"): The name of a specific tool the model must use.\n   * - An object: A custom schema specifying tool choice parameters. Specific to the provider.\n   *\n   * Note: Not all providers support tool_choice. An error will be thrown\n   * if used with an unsupported model.\n   */\n  tool_choice?: ToolChoice;\n  /**\n   * Version of `AIMessage` output format to store in message content.\n   *\n   * `AIMessage.contentBlocks` will lazily parse the contents of `content` into a\n   * standard format. This flag can be used to additionally store the standard format\n   * as the message content, e.g., for serialization purposes.\n   *\n   * - \"v0\": provider-specific format in content (can lazily parse with `.contentBlocks`)\n   * - \"v1\": standardized format in content (consistent with `.contentBlocks`)\n   *\n   * You can also set `LC_OUTPUT_VERSION` as an environment variable to \"v1\" to\n   * enable this by default.\n   *\n   * @default \"v0\"\n   */\n  outputVersion?: MessageOutputVersion;\n};\n\nfunction _formatForTracing(messages: BaseMessage[]): BaseMessage[] {\n  const messagesToTrace: BaseMessage[] = [];\n  for (const message of messages) {\n    let messageToTrace = message;\n    if (Array.isArray(message.content)) {\n      for (let idx = 0; idx < message.content.length; idx++) {\n        const block = message.content[idx];\n        if (isURLContentBlock(block) || isBase64ContentBlock(block)) {\n          if (messageToTrace === message) {\n            // Also shallow-copy content\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            messageToTrace = new (message.constructor as any)({\n              ...messageToTrace,\n              content: [\n                ...message.content.slice(0, idx),\n                convertToOpenAIImageBlock(block),\n                ...message.content.slice(idx + 1),\n              ],\n            });\n          }\n        }\n      }\n    }\n    messagesToTrace.push(messageToTrace);\n  }\n  return messagesToTrace;\n}\n\nexport type LangSmithParams = {\n  ls_provider?: string;\n  ls_model_name?: string;\n  ls_model_type: \"chat\";\n  ls_temperature?: number;\n  ls_max_tokens?: number;\n  ls_stop?: Array<string>;\n};\n\nexport type BindToolsInput =\n  | StructuredToolInterface\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  | Record<string, any>\n  | ToolDefinition\n  | RunnableToolLike\n  | StructuredToolParams;\n\n/**\n * Base class for chat models. It extends the BaseLanguageModel class and\n * provides methods for generating chat based on input messages.\n */\nexport abstract class BaseChatModel<\n  CallOptions extends BaseChatModelCallOptions = BaseChatModelCallOptions,\n  // TODO: Fix the parameter order on the next minor version.\n  OutputMessageType extends BaseMessageChunk = AIMessageChunk,\n> extends BaseLanguageModel<OutputMessageType, CallOptions> {\n  // Backwards compatibility since fields have been moved to RunnableConfig\n  declare ParsedCallOptions: Omit<\n    CallOptions,\n    Exclude<keyof RunnableConfig, \"signal\" | \"timeout\" | \"maxConcurrency\">\n  >;\n\n  // Only ever instantiated in main LangChain\n  lc_namespace = [\"langchain\", \"chat_models\", this._llmType()];\n\n  disableStreaming = false;\n\n  outputVersion?: MessageOutputVersion;\n\n  get callKeys(): string[] {\n    return [...super.callKeys, \"outputVersion\"];\n  }\n\n  constructor(fields: BaseChatModelParams) {\n    super(fields);\n    this.outputVersion = iife(() => {\n      const outputVersion =\n        fields.outputVersion ?? getEnvironmentVariable(\"LC_OUTPUT_VERSION\");\n      if (outputVersion && [\"v0\", \"v1\"].includes(outputVersion)) {\n        return outputVersion as \"v0\" | \"v1\";\n      }\n      return \"v0\";\n    });\n  }\n\n  _combineLLMOutput?(\n    ...llmOutputs: LLMResult[\"llmOutput\"][]\n  ): LLMResult[\"llmOutput\"];\n\n  protected _separateRunnableConfigFromCallOptionsCompat(\n    options?: Partial<CallOptions>\n  ): [RunnableConfig, this[\"ParsedCallOptions\"]] {\n    // For backwards compat, keep `signal` in both runnableConfig and callOptions\n    const [runnableConfig, callOptions] =\n      super._separateRunnableConfigFromCallOptions(options);\n    (callOptions as this[\"ParsedCallOptions\"]).signal = runnableConfig.signal;\n    return [runnableConfig, callOptions as this[\"ParsedCallOptions\"]];\n  }\n\n  /**\n   * Bind tool-like objects to this chat model.\n   *\n   * @param tools A list of tool definitions to bind to this chat model.\n   * Can be a structured tool, an OpenAI formatted tool, or an object\n   * matching the provider's specific tool schema.\n   * @param kwargs Any additional parameters to bind.\n   */\n  bindTools?(\n    tools: BindToolsInput[],\n    kwargs?: Partial<CallOptions>\n  ): Runnable<BaseLanguageModelInput, OutputMessageType, CallOptions>;\n\n  /**\n   * Invokes the chat model with a single input.\n   * @param input The input for the language model.\n   * @param options The call options.\n   * @returns A Promise that resolves to a BaseMessageChunk.\n   */\n  async invoke(\n    input: BaseLanguageModelInput,\n    options?: CallOptions\n  ): Promise<OutputMessageType> {\n    const promptValue = BaseChatModel._convertInputToPromptValue(input);\n    const result = await this.generatePrompt(\n      [promptValue],\n      options,\n      options?.callbacks\n    );\n    const chatGeneration = result.generations[0][0] as ChatGeneration;\n    // TODO: Remove cast after figuring out inheritance\n    return chatGeneration.message as OutputMessageType;\n  }\n\n  // eslint-disable-next-line require-yield\n  async *_streamResponseChunks(\n    _messages: BaseMessage[],\n    _options: this[\"ParsedCallOptions\"],\n    _runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    throw new Error(\"Not implemented.\");\n  }\n\n  async *_streamIterator(\n    input: BaseLanguageModelInput,\n    options?: CallOptions\n  ): AsyncGenerator<OutputMessageType> {\n    // Subclass check required to avoid double callbacks with default implementation\n    if (\n      this._streamResponseChunks ===\n        BaseChatModel.prototype._streamResponseChunks ||\n      this.disableStreaming\n    ) {\n      yield this.invoke(input, options);\n    } else {\n      const prompt = BaseChatModel._convertInputToPromptValue(input);\n      const messages = prompt.toChatMessages();\n      const [runnableConfig, callOptions] =\n        this._separateRunnableConfigFromCallOptionsCompat(options);\n\n      const inheritableMetadata = {\n        ...runnableConfig.metadata,\n        ...this.getLsParams(callOptions),\n      };\n      const callbackManager_ = await CallbackManager.configure(\n        runnableConfig.callbacks,\n        this.callbacks,\n        runnableConfig.tags,\n        this.tags,\n        inheritableMetadata,\n        this.metadata,\n        { verbose: this.verbose }\n      );\n      const extra = {\n        options: callOptions,\n        invocation_params: this?.invocationParams(callOptions),\n        batch_size: 1,\n      };\n      const outputVersion = callOptions.outputVersion ?? this.outputVersion;\n      const runManagers = await callbackManager_?.handleChatModelStart(\n        this.toJSON(),\n        [_formatForTracing(messages)],\n        runnableConfig.runId,\n        undefined,\n        extra,\n        undefined,\n        undefined,\n        runnableConfig.runName\n      );\n      let generationChunk: ChatGenerationChunk | undefined;\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      let llmOutput: Record<string, any> | undefined;\n      try {\n        for await (const chunk of this._streamResponseChunks(\n          messages,\n          callOptions,\n          runManagers?.[0]\n        )) {\n          if (chunk.message.id == null) {\n            const runId = runManagers?.at(0)?.runId;\n            if (runId != null) chunk.message._updateId(`run-${runId}`);\n          }\n          chunk.message.response_metadata = {\n            ...chunk.generationInfo,\n            ...chunk.message.response_metadata,\n          };\n          if (outputVersion === \"v1\") {\n            yield castStandardMessageContent(\n              chunk.message\n            ) as OutputMessageType;\n          } else {\n            yield chunk.message as OutputMessageType;\n          }\n          if (!generationChunk) {\n            generationChunk = chunk;\n          } else {\n            generationChunk = generationChunk.concat(chunk);\n          }\n          if (\n            isAIMessageChunk(chunk.message) &&\n            chunk.message.usage_metadata !== undefined\n          ) {\n            llmOutput = {\n              tokenUsage: {\n                promptTokens: chunk.message.usage_metadata.input_tokens,\n                completionTokens: chunk.message.usage_metadata.output_tokens,\n                totalTokens: chunk.message.usage_metadata.total_tokens,\n              },\n            };\n          }\n        }\n      } catch (err) {\n        await Promise.all(\n          (runManagers ?? []).map((runManager) =>\n            runManager?.handleLLMError(err)\n          )\n        );\n        throw err;\n      }\n      await Promise.all(\n        (runManagers ?? []).map((runManager) =>\n          runManager?.handleLLMEnd({\n            // TODO: Remove cast after figuring out inheritance\n            generations: [[generationChunk as ChatGeneration]],\n            llmOutput,\n          })\n        )\n      );\n    }\n  }\n\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams {\n    const providerName = this.getName().startsWith(\"Chat\")\n      ? this.getName().replace(\"Chat\", \"\")\n      : this.getName();\n\n    return {\n      ls_model_type: \"chat\",\n      ls_stop: options.stop,\n      ls_provider: providerName,\n    };\n  }\n\n  /** @ignore */\n  async _generateUncached(\n    messages: BaseMessageLike[][],\n    parsedOptions: this[\"ParsedCallOptions\"],\n    handledOptions: RunnableConfig,\n    startedRunManagers?: CallbackManagerForLLMRun[]\n  ): Promise<LLMResult> {\n    const baseMessages = messages.map((messageList) =>\n      messageList.map(coerceMessageLikeToMessage)\n    );\n\n    let runManagers: CallbackManagerForLLMRun[] | undefined;\n    if (\n      startedRunManagers !== undefined &&\n      startedRunManagers.length === baseMessages.length\n    ) {\n      runManagers = startedRunManagers;\n    } else {\n      const inheritableMetadata = {\n        ...handledOptions.metadata,\n        ...this.getLsParams(parsedOptions),\n      };\n      // create callback manager and start run\n      const callbackManager_ = await CallbackManager.configure(\n        handledOptions.callbacks,\n        this.callbacks,\n        handledOptions.tags,\n        this.tags,\n        inheritableMetadata,\n        this.metadata,\n        { verbose: this.verbose }\n      );\n      const extra = {\n        options: parsedOptions,\n        invocation_params: this?.invocationParams(parsedOptions),\n        batch_size: 1,\n      };\n      runManagers = await callbackManager_?.handleChatModelStart(\n        this.toJSON(),\n        baseMessages.map(_formatForTracing),\n        handledOptions.runId,\n        undefined,\n        extra,\n        undefined,\n        undefined,\n        handledOptions.runName\n      );\n    }\n    const outputVersion = parsedOptions.outputVersion ?? this.outputVersion;\n    const generations: ChatGeneration[][] = [];\n    const llmOutputs: LLMResult[\"llmOutput\"][] = [];\n    // Even if stream is not explicitly called, check if model is implicitly\n    // called from streamEvents() or streamLog() to get all streamed events.\n    // Bail out if _streamResponseChunks not overridden\n    const hasStreamingHandler = !!runManagers?.[0].handlers.find(\n      callbackHandlerPrefersStreaming\n    );\n    if (\n      hasStreamingHandler &&\n      !this.disableStreaming &&\n      baseMessages.length === 1 &&\n      this._streamResponseChunks !==\n        BaseChatModel.prototype._streamResponseChunks\n    ) {\n      try {\n        const stream = await this._streamResponseChunks(\n          baseMessages[0],\n          parsedOptions,\n          runManagers?.[0]\n        );\n        let aggregated;\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        let llmOutput: Record<string, any> | undefined;\n        for await (const chunk of stream) {\n          if (chunk.message.id == null) {\n            const runId = runManagers?.at(0)?.runId;\n            if (runId != null) chunk.message._updateId(`run-${runId}`);\n          }\n          if (aggregated === undefined) {\n            aggregated = chunk;\n          } else {\n            aggregated = concat(aggregated, chunk);\n          }\n          if (\n            isAIMessageChunk(chunk.message) &&\n            chunk.message.usage_metadata !== undefined\n          ) {\n            llmOutput = {\n              tokenUsage: {\n                promptTokens: chunk.message.usage_metadata.input_tokens,\n                completionTokens: chunk.message.usage_metadata.output_tokens,\n                totalTokens: chunk.message.usage_metadata.total_tokens,\n              },\n            };\n          }\n        }\n        if (aggregated === undefined) {\n          throw new Error(\"Received empty response from chat model call.\");\n        }\n        generations.push([aggregated]);\n        await runManagers?.[0].handleLLMEnd({\n          generations,\n          llmOutput,\n        });\n      } catch (e) {\n        await runManagers?.[0].handleLLMError(e);\n        throw e;\n      }\n    } else {\n      // generate results\n      const results = await Promise.allSettled(\n        baseMessages.map(async (messageList, i) => {\n          const generateResults = await this._generate(\n            messageList,\n            { ...parsedOptions, promptIndex: i },\n            runManagers?.[i]\n          );\n          if (outputVersion === \"v1\") {\n            for (const generation of generateResults.generations) {\n              generation.message = castStandardMessageContent(\n                generation.message\n              );\n            }\n          }\n          return generateResults;\n        })\n      );\n      // handle results\n      await Promise.all(\n        results.map(async (pResult, i) => {\n          if (pResult.status === \"fulfilled\") {\n            const result = pResult.value;\n            for (const generation of result.generations) {\n              if (generation.message.id == null) {\n                const runId = runManagers?.at(0)?.runId;\n                if (runId != null) generation.message._updateId(`run-${runId}`);\n              }\n              generation.message.response_metadata = {\n                ...generation.generationInfo,\n                ...generation.message.response_metadata,\n              };\n            }\n            if (result.generations.length === 1) {\n              result.generations[0].message.response_metadata = {\n                ...result.llmOutput,\n                ...result.generations[0].message.response_metadata,\n              };\n            }\n            generations[i] = result.generations;\n            llmOutputs[i] = result.llmOutput;\n            return runManagers?.[i]?.handleLLMEnd({\n              generations: [result.generations],\n              llmOutput: result.llmOutput,\n            });\n          } else {\n            // status === \"rejected\"\n            await runManagers?.[i]?.handleLLMError(pResult.reason);\n            return Promise.reject(pResult.reason);\n          }\n        })\n      );\n    }\n    // create combined output\n    const output: LLMResult = {\n      generations,\n      llmOutput: llmOutputs.length\n        ? this._combineLLMOutput?.(...llmOutputs)\n        : undefined,\n    };\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers\n        ? { runIds: runManagers?.map((manager) => manager.runId) }\n        : undefined,\n      configurable: true,\n    });\n    return output;\n  }\n\n  async _generateCached({\n    messages,\n    cache,\n    llmStringKey,\n    parsedOptions,\n    handledOptions,\n  }: {\n    messages: BaseMessageLike[][];\n    cache: BaseCache<Generation[]>;\n    llmStringKey: string;\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    parsedOptions: any;\n    handledOptions: RunnableConfig;\n  }): Promise<\n    LLMResult & {\n      missingPromptIndices: number[];\n      startedRunManagers?: CallbackManagerForLLMRun[];\n    }\n  > {\n    const baseMessages = messages.map((messageList) =>\n      messageList.map(coerceMessageLikeToMessage)\n    );\n\n    const inheritableMetadata = {\n      ...handledOptions.metadata,\n      ...this.getLsParams(parsedOptions),\n    };\n    // create callback manager and start run\n    const callbackManager_ = await CallbackManager.configure(\n      handledOptions.callbacks,\n      this.callbacks,\n      handledOptions.tags,\n      this.tags,\n      inheritableMetadata,\n      this.metadata,\n      { verbose: this.verbose }\n    );\n    const extra = {\n      options: parsedOptions,\n      invocation_params: this?.invocationParams(parsedOptions),\n      batch_size: 1,\n    };\n    const runManagers = await callbackManager_?.handleChatModelStart(\n      this.toJSON(),\n      baseMessages.map(_formatForTracing),\n      handledOptions.runId,\n      undefined,\n      extra,\n      undefined,\n      undefined,\n      handledOptions.runName\n    );\n\n    // generate results\n    const missingPromptIndices: number[] = [];\n    const results = await Promise.allSettled(\n      baseMessages.map(async (baseMessage, index) => {\n        // Join all content into one string for the prompt index\n        const prompt =\n          BaseChatModel._convertInputToPromptValue(baseMessage).toString();\n        const result = await cache.lookup(prompt, llmStringKey);\n\n        if (result == null) {\n          missingPromptIndices.push(index);\n        }\n\n        return result;\n      })\n    );\n\n    // Map run managers to the results before filtering out null results\n    // Null results are just absent from the cache.\n    const cachedResults = results\n      .map((result, index) => ({ result, runManager: runManagers?.[index] }))\n      .filter(\n        ({ result }) =>\n          (result.status === \"fulfilled\" && result.value != null) ||\n          result.status === \"rejected\"\n      );\n\n    // Handle results and call run managers\n    const outputVersion = parsedOptions.outputVersion ?? this.outputVersion;\n    const generations: Generation[][] = [];\n    await Promise.all(\n      cachedResults.map(async ({ result: promiseResult, runManager }, i) => {\n        if (promiseResult.status === \"fulfilled\") {\n          const result = promiseResult.value as Generation[];\n          generations[i] = result.map((result) => {\n            if (\n              \"message\" in result &&\n              isBaseMessage(result.message) &&\n              isAIMessage(result.message)\n            ) {\n              result.message.usage_metadata = {\n                input_tokens: 0,\n                output_tokens: 0,\n                total_tokens: 0,\n              };\n              if (outputVersion === \"v1\") {\n                result.message = castStandardMessageContent(result.message);\n              }\n            }\n            result.generationInfo = {\n              ...result.generationInfo,\n              tokenUsage: {},\n            };\n            return result;\n          });\n          if (result.length) {\n            await runManager?.handleLLMNewToken(result[0].text);\n          }\n          return runManager?.handleLLMEnd(\n            {\n              generations: [result],\n            },\n            undefined,\n            undefined,\n            undefined,\n            {\n              cached: true,\n            }\n          );\n        } else {\n          // status === \"rejected\"\n          await runManager?.handleLLMError(\n            promiseResult.reason,\n            undefined,\n            undefined,\n            undefined,\n            {\n              cached: true,\n            }\n          );\n          return Promise.reject(promiseResult.reason);\n        }\n      })\n    );\n\n    const output = {\n      generations,\n      missingPromptIndices,\n      startedRunManagers: runManagers,\n    };\n\n    // This defines RUN_KEY as a non-enumerable property on the output object\n    // so that it is not serialized when the output is stringified, and so that\n    // it isnt included when listing the keys of the output object.\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers\n        ? { runIds: runManagers?.map((manager) => manager.runId) }\n        : undefined,\n      configurable: true,\n    });\n\n    return output;\n  }\n\n  /**\n   * Generates chat based on the input messages.\n   * @param messages An array of arrays of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to an LLMResult.\n   */\n  async generate(\n    messages: BaseMessageLike[][],\n    options?: string[] | CallOptions,\n    callbacks?: Callbacks\n  ): Promise<LLMResult> {\n    // parse call options\n    let parsedOptions: CallOptions | undefined;\n    if (Array.isArray(options)) {\n      parsedOptions = { stop: options } as CallOptions;\n    } else {\n      parsedOptions = options;\n    }\n\n    const baseMessages = messages.map((messageList) =>\n      messageList.map(coerceMessageLikeToMessage)\n    );\n\n    const [runnableConfig, callOptions] =\n      this._separateRunnableConfigFromCallOptionsCompat(parsedOptions);\n    runnableConfig.callbacks = runnableConfig.callbacks ?? callbacks;\n\n    if (!this.cache) {\n      return this._generateUncached(baseMessages, callOptions, runnableConfig);\n    }\n\n    const { cache } = this;\n    const llmStringKey = this._getSerializedCacheKeyParametersForCall(\n      callOptions as CallOptions\n    );\n\n    const { generations, missingPromptIndices, startedRunManagers } =\n      await this._generateCached({\n        messages: baseMessages,\n        cache,\n        llmStringKey,\n        parsedOptions: callOptions,\n        handledOptions: runnableConfig,\n      });\n\n    let llmOutput = {};\n    if (missingPromptIndices.length > 0) {\n      const results = await this._generateUncached(\n        missingPromptIndices.map((i) => baseMessages[i]),\n        callOptions,\n        runnableConfig,\n        startedRunManagers !== undefined\n          ? missingPromptIndices.map((i) => startedRunManagers?.[i])\n          : undefined\n      );\n      await Promise.all(\n        results.generations.map(async (generation, index) => {\n          const promptIndex = missingPromptIndices[index];\n          generations[promptIndex] = generation;\n          // Join all content into one string for the prompt index\n          const prompt = BaseChatModel._convertInputToPromptValue(\n            baseMessages[promptIndex]\n          ).toString();\n          return cache.update(prompt, llmStringKey, generation);\n        })\n      );\n      llmOutput = results.llmOutput ?? {};\n    }\n\n    return { generations, llmOutput } as LLMResult;\n  }\n\n  /**\n   * Get the parameters used to invoke the model\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  invocationParams(_options?: this[\"ParsedCallOptions\"]): any {\n    return {};\n  }\n\n  _modelType(): string {\n    return \"base_chat_model\" as const;\n  }\n\n  abstract _llmType(): string;\n\n  /**\n   * Generates a prompt based on the input prompt values.\n   * @param promptValues An array of BasePromptValue instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to an LLMResult.\n   */\n  async generatePrompt(\n    promptValues: BasePromptValueInterface[],\n    options?: string[] | CallOptions,\n    callbacks?: Callbacks\n  ): Promise<LLMResult> {\n    const promptMessages: BaseMessage[][] = promptValues.map((promptValue) =>\n      promptValue.toChatMessages()\n    );\n    return this.generate(promptMessages, options, callbacks);\n  }\n\n  abstract _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    outputSchema:\n      | ZodTypeV4<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<false>\n  ): Runnable<BaseLanguageModelInput, RunOutput>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    outputSchema:\n      | ZodTypeV4<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<true>\n  ): Runnable<BaseLanguageModelInput, { raw: BaseMessage; parsed: RunOutput }>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    outputSchema:\n      | ZodTypeV3<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<false>\n  ): Runnable<BaseLanguageModelInput, RunOutput>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    outputSchema:\n      | ZodTypeV3<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<true>\n  ): Runnable<BaseLanguageModelInput, { raw: BaseMessage; parsed: RunOutput }>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<boolean>\n  ):\n    | Runnable<BaseLanguageModelInput, RunOutput>\n    | Runnable<\n        BaseLanguageModelInput,\n        {\n          raw: BaseMessage;\n          parsed: RunOutput;\n        }\n      > {\n    if (typeof this.bindTools !== \"function\") {\n      throw new Error(\n        `Chat model must implement \".bindTools()\" to use withStructuredOutput.`\n      );\n    }\n    if (config?.strict) {\n      throw new Error(\n        `\"strict\" mode is not supported for this model by default.`\n      );\n    }\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    const schema: Record<string, any> | InteropZodType<RunOutput> =\n      outputSchema;\n    const name = config?.name;\n    const description =\n      getSchemaDescription(schema) ?? \"A function available to call.\";\n    const method = config?.method;\n    const includeRaw = config?.includeRaw;\n    if (method === \"jsonMode\") {\n      throw new Error(\n        `Base withStructuredOutput implementation only supports \"functionCalling\" as a method.`\n      );\n    }\n\n    let functionName = name ?? \"extract\";\n    let tools: ToolDefinition[];\n    if (isInteropZodSchema(schema)) {\n      tools = [\n        {\n          type: \"function\",\n          function: {\n            name: functionName,\n            description,\n            parameters: toJsonSchema(schema),\n          },\n        },\n      ];\n    } else {\n      if (\"name\" in schema) {\n        functionName = schema.name;\n      }\n      tools = [\n        {\n          type: \"function\",\n          function: {\n            name: functionName,\n            description,\n            parameters: schema,\n          },\n        },\n      ];\n    }\n\n    const llm = this.bindTools(tools);\n    const outputParser = RunnableLambda.from<OutputMessageType, RunOutput>(\n      (input: BaseMessageChunk): RunOutput => {\n        if (!AIMessageChunk.isInstance(input)) {\n          throw new Error(\"Input is not an AIMessageChunk.\");\n        }\n        if (!input.tool_calls || input.tool_calls.length === 0) {\n          throw new Error(\"No tool calls found in the response.\");\n        }\n        const toolCall = input.tool_calls.find(\n          (tc) => tc.name === functionName\n        );\n        if (!toolCall) {\n          throw new Error(`No tool call found with name ${functionName}.`);\n        }\n        return toolCall.args as RunOutput;\n      }\n    );\n\n    if (!includeRaw) {\n      return llm.pipe(outputParser).withConfig({\n        runName: \"StructuredOutput\",\n      }) as Runnable<BaseLanguageModelInput, RunOutput>;\n    }\n\n    const parserAssign = RunnablePassthrough.assign({\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      parsed: (input: any, config) => outputParser.invoke(input.raw, config),\n    });\n    const parserNone = RunnablePassthrough.assign({\n      parsed: () => null,\n    });\n    const parsedWithFallback = parserAssign.withFallbacks({\n      fallbacks: [parserNone],\n    });\n    return RunnableSequence.from<\n      BaseLanguageModelInput,\n      { raw: BaseMessage; parsed: RunOutput }\n    >([\n      {\n        raw: llm,\n      },\n      parsedWithFallback,\n    ]).withConfig({\n      runName: \"StructuredOutputRunnable\",\n    });\n  }\n}\n\n/**\n * An abstract class that extends BaseChatModel and provides a simple\n * implementation of _generate.\n */\nexport abstract class SimpleChatModel<\n  CallOptions extends BaseChatModelCallOptions = BaseChatModelCallOptions,\n> extends BaseChatModel<CallOptions> {\n  abstract _call(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<string>;\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    const text = await this._call(messages, options, runManager);\n    const message = new AIMessage(text);\n    if (typeof message.content !== \"string\") {\n      throw new Error(\n        \"Cannot generate with a simple chat model when output is not a string.\"\n      );\n    }\n    return {\n      generations: [\n        {\n          text: message.content,\n          message,\n        },\n      ],\n    };\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;AA4JA,SAAS,kBAAkBA,UAAwC;CACjE,MAAMC,kBAAiC,CAAE;AACzC,MAAK,MAAM,WAAW,UAAU;EAC9B,IAAI,iBAAiB;AACrB,MAAI,MAAM,QAAQ,QAAQ,QAAQ,CAChC,MAAK,IAAI,MAAM,GAAG,MAAM,QAAQ,QAAQ,QAAQ,OAAO;GACrD,MAAM,QAAQ,QAAQ,QAAQ;AAC9B,OAAIC,+BAAkB,MAAM,IAAIC,kCAAqB,MAAM,EACzD;QAAI,mBAAmB,SAGrB,iBAAiB,IAAK,QAAQ,YAAoB;KAChD,GAAG;KACH,SAAS;MACP,GAAG,QAAQ,QAAQ,MAAM,GAAG,IAAI;MAChCC,uCAA0B,MAAM;MAChC,GAAG,QAAQ,QAAQ,MAAM,MAAM,EAAE;KAClC;IACF;GACF;EAEJ;EAEH,gBAAgB,KAAK,eAAe;CACrC;AACD,QAAO;AACR;;;;;AAuBD,IAAsB,gBAAtB,MAAsB,sBAIZC,+CAAkD;CAQ1D,eAAe;EAAC;EAAa;EAAe,KAAK,UAAU;CAAC;CAE5D,mBAAmB;CAEnB;CAEA,IAAI,WAAqB;AACvB,SAAO,CAAC,GAAG,MAAM,UAAU,eAAgB;CAC5C;CAED,YAAYC,QAA6B;EACvC,MAAM,OAAO;EACb,KAAK,gBAAgBC,qBAAK,MAAM;GAC9B,MAAM,gBACJ,OAAO,iBAAiBC,yCAAuB,oBAAoB;AACrE,OAAI,iBAAiB,CAAC,MAAM,IAAK,EAAC,SAAS,cAAc,CACvD,QAAO;AAET,UAAO;EACR,EAAC;CACH;CAMD,AAAU,6CACRC,SAC6C;EAE7C,MAAM,CAAC,gBAAgB,YAAY,GACjC,MAAM,uCAAuC,QAAQ;EACtD,YAA0C,SAAS,eAAe;AACnE,SAAO,CAAC,gBAAgB,WAAyC;CAClE;;;;;;;CAqBD,MAAM,OACJC,OACAC,SAC4B;EAC5B,MAAM,cAAc,cAAc,2BAA2B,MAAM;EACnE,MAAM,SAAS,MAAM,KAAK,eACxB,CAAC,WAAY,GACb,SACA,SAAS,UACV;EACD,MAAM,iBAAiB,OAAO,YAAY,GAAG;AAE7C,SAAO,eAAe;CACvB;CAGD,OAAO,sBACLC,WACAC,UACAC,aACqC;AACrC,QAAM,IAAI,MAAM;CACjB;CAED,OAAO,gBACLJ,OACAC,SACmC;AAEnC,MACE,KAAK,0BACH,cAAc,UAAU,yBAC1B,KAAK,kBAEL,MAAM,KAAK,OAAO,OAAO,QAAQ;OAC5B;GACL,MAAM,SAAS,cAAc,2BAA2B,MAAM;GAC9D,MAAM,WAAW,OAAO,gBAAgB;GACxC,MAAM,CAAC,gBAAgB,YAAY,GACjC,KAAK,6CAA6C,QAAQ;GAE5D,MAAM,sBAAsB;IAC1B,GAAG,eAAe;IAClB,GAAG,KAAK,YAAY,YAAY;GACjC;GACD,MAAM,mBAAmB,MAAMI,0CAAgB,UAC7C,eAAe,WACf,KAAK,WACL,eAAe,MACf,KAAK,MACL,qBACA,KAAK,UACL,EAAE,SAAS,KAAK,QAAS,EAC1B;GACD,MAAM,QAAQ;IACZ,SAAS;IACT,mBAAmB,MAAM,iBAAiB,YAAY;IACtD,YAAY;GACb;GACD,MAAM,gBAAgB,YAAY,iBAAiB,KAAK;GACxD,MAAM,cAAc,MAAM,kBAAkB,qBAC1C,KAAK,QAAQ,EACb,CAAC,kBAAkB,SAAS,AAAC,GAC7B,eAAe,OACf,QACA,OACA,QACA,QACA,eAAe,QAChB;GACD,IAAIC;GAEJ,IAAIC;AACJ,OAAI;AACF,eAAW,MAAM,SAAS,KAAK,sBAC7B,UACA,aACA,cAAc,GACf,EAAE;AACD,SAAI,MAAM,QAAQ,MAAM,MAAM;MAC5B,MAAM,QAAQ,aAAa,GAAG,EAAE,EAAE;AAClC,UAAI,SAAS,MAAM,MAAM,QAAQ,UAAU,CAAC,IAAI,EAAE,OAAO,CAAC;KAC3D;KACD,MAAM,QAAQ,oBAAoB;MAChC,GAAG,MAAM;MACT,GAAG,MAAM,QAAQ;KAClB;AACD,SAAI,kBAAkB,MACpB,MAAMC,2CACJ,MAAM,QACP;UAED,MAAM,MAAM;AAEd,SAAI,CAAC,iBACH,kBAAkB;UAElB,kBAAkB,gBAAgB,OAAO,MAAM;AAEjD,SACEC,4BAAiB,MAAM,QAAQ,IAC/B,MAAM,QAAQ,mBAAmB,QAEjC,YAAY,EACV,YAAY;MACV,cAAc,MAAM,QAAQ,eAAe;MAC3C,kBAAkB,MAAM,QAAQ,eAAe;MAC/C,aAAa,MAAM,QAAQ,eAAe;KAC3C,EACF;IAEJ;GACF,SAAQ,KAAK;IACZ,MAAM,QAAQ,KACX,eAAe,CAAE,GAAE,IAAI,CAAC,eACvB,YAAY,eAAe,IAAI,CAChC,CACF;AACD,UAAM;GACP;GACD,MAAM,QAAQ,KACX,eAAe,CAAE,GAAE,IAAI,CAAC,eACvB,YAAY,aAAa;IAEvB,aAAa,CAAC,CAAC,eAAkC,CAAC;IAClD;GACD,EAAC,CACH,CACF;EACF;CACF;CAED,YAAYC,SAAqD;EAC/D,MAAM,eAAe,KAAK,SAAS,CAAC,WAAW,OAAO,GAClD,KAAK,SAAS,CAAC,QAAQ,QAAQ,GAAG,GAClC,KAAK,SAAS;AAElB,SAAO;GACL,eAAe;GACf,SAAS,QAAQ;GACjB,aAAa;EACd;CACF;;CAGD,MAAM,kBACJC,UACAC,eACAC,gBACAC,oBACoB;EACpB,MAAM,eAAe,SAAS,IAAI,CAAC,gBACjC,YAAY,IAAIC,yCAA2B,CAC5C;EAED,IAAIC;AACJ,MACE,uBAAuB,UACvB,mBAAmB,WAAW,aAAa,QAE3C,cAAc;OACT;GACL,MAAM,sBAAsB;IAC1B,GAAG,eAAe;IAClB,GAAG,KAAK,YAAY,cAAc;GACnC;GAED,MAAM,mBAAmB,MAAMX,0CAAgB,UAC7C,eAAe,WACf,KAAK,WACL,eAAe,MACf,KAAK,MACL,qBACA,KAAK,UACL,EAAE,SAAS,KAAK,QAAS,EAC1B;GACD,MAAM,QAAQ;IACZ,SAAS;IACT,mBAAmB,MAAM,iBAAiB,cAAc;IACxD,YAAY;GACb;GACD,cAAc,MAAM,kBAAkB,qBACpC,KAAK,QAAQ,EACb,aAAa,IAAI,kBAAkB,EACnC,eAAe,OACf,QACA,OACA,QACA,QACA,eAAe,QAChB;EACF;EACD,MAAM,gBAAgB,cAAc,iBAAiB,KAAK;EAC1D,MAAMY,cAAkC,CAAE;EAC1C,MAAMC,aAAuC,CAAE;EAI/C,MAAM,sBAAsB,CAAC,CAAC,cAAc,GAAG,SAAS,KACtDC,uDACD;AACD,MACE,uBACA,CAAC,KAAK,oBACN,aAAa,WAAW,KACxB,KAAK,0BACH,cAAc,UAAU,sBAE1B,KAAI;GACF,MAAM,SAAS,MAAM,KAAK,sBACxB,aAAa,IACb,eACA,cAAc,GACf;GACD,IAAI;GAEJ,IAAIZ;AACJ,cAAW,MAAM,SAAS,QAAQ;AAChC,QAAI,MAAM,QAAQ,MAAM,MAAM;KAC5B,MAAM,QAAQ,aAAa,GAAG,EAAE,EAAE;AAClC,SAAI,SAAS,MAAM,MAAM,QAAQ,UAAU,CAAC,IAAI,EAAE,OAAO,CAAC;IAC3D;AACD,QAAI,eAAe,QACjB,aAAa;SAEb,aAAaa,4BAAO,YAAY,MAAM;AAExC,QACEX,4BAAiB,MAAM,QAAQ,IAC/B,MAAM,QAAQ,mBAAmB,QAEjC,YAAY,EACV,YAAY;KACV,cAAc,MAAM,QAAQ,eAAe;KAC3C,kBAAkB,MAAM,QAAQ,eAAe;KAC/C,aAAa,MAAM,QAAQ,eAAe;IAC3C,EACF;GAEJ;AACD,OAAI,eAAe,OACjB,OAAM,IAAI,MAAM;GAElB,YAAY,KAAK,CAAC,UAAW,EAAC;GAC9B,MAAM,cAAc,GAAG,aAAa;IAClC;IACA;GACD,EAAC;EACH,SAAQ,GAAG;GACV,MAAM,cAAc,GAAG,eAAe,EAAE;AACxC,SAAM;EACP;OACI;GAEL,MAAM,UAAU,MAAM,QAAQ,WAC5B,aAAa,IAAI,OAAO,aAAa,MAAM;IACzC,MAAM,kBAAkB,MAAM,KAAK,UACjC,aACA;KAAE,GAAG;KAAe,aAAa;IAAG,GACpC,cAAc,GACf;AACD,QAAI,kBAAkB,KACpB,MAAK,MAAM,cAAc,gBAAgB,aACvC,WAAW,UAAUD,2CACnB,WAAW,QACZ;AAGL,WAAO;GACR,EAAC,CACH;GAED,MAAM,QAAQ,IACZ,QAAQ,IAAI,OAAO,SAAS,MAAM;AAChC,QAAI,QAAQ,WAAW,aAAa;KAClC,MAAM,SAAS,QAAQ;AACvB,UAAK,MAAM,cAAc,OAAO,aAAa;AAC3C,UAAI,WAAW,QAAQ,MAAM,MAAM;OACjC,MAAM,QAAQ,aAAa,GAAG,EAAE,EAAE;AAClC,WAAI,SAAS,MAAM,WAAW,QAAQ,UAAU,CAAC,IAAI,EAAE,OAAO,CAAC;MAChE;MACD,WAAW,QAAQ,oBAAoB;OACrC,GAAG,WAAW;OACd,GAAG,WAAW,QAAQ;MACvB;KACF;AACD,SAAI,OAAO,YAAY,WAAW,GAChC,OAAO,YAAY,GAAG,QAAQ,oBAAoB;MAChD,GAAG,OAAO;MACV,GAAG,OAAO,YAAY,GAAG,QAAQ;KAClC;KAEH,YAAY,KAAK,OAAO;KACxB,WAAW,KAAK,OAAO;AACvB,YAAO,cAAc,IAAI,aAAa;MACpC,aAAa,CAAC,OAAO,WAAY;MACjC,WAAW,OAAO;KACnB,EAAC;IACH,OAAM;KAEL,MAAM,cAAc,IAAI,eAAe,QAAQ,OAAO;AACtD,YAAO,QAAQ,OAAO,QAAQ,OAAO;IACtC;GACF,EAAC,CACH;EACF;EAED,MAAMa,SAAoB;GACxB;GACA,WAAW,WAAW,SAClB,KAAK,oBAAoB,GAAG,WAAW,GACvC;EACL;EACD,OAAO,eAAe,QAAQC,yBAAS;GACrC,OAAO,cACH,EAAE,QAAQ,aAAa,IAAI,CAAC,YAAY,QAAQ,MAAM,CAAE,IACxD;GACJ,cAAc;EACf,EAAC;AACF,SAAO;CACR;CAED,MAAM,gBAAgB,EACpB,UACA,OACA,cACA,eACA,gBAQD,EAKC;EACA,MAAM,eAAe,SAAS,IAAI,CAAC,gBACjC,YAAY,IAAIP,yCAA2B,CAC5C;EAED,MAAM,sBAAsB;GAC1B,GAAG,eAAe;GAClB,GAAG,KAAK,YAAY,cAAc;EACnC;EAED,MAAM,mBAAmB,MAAMV,0CAAgB,UAC7C,eAAe,WACf,KAAK,WACL,eAAe,MACf,KAAK,MACL,qBACA,KAAK,UACL,EAAE,SAAS,KAAK,QAAS,EAC1B;EACD,MAAM,QAAQ;GACZ,SAAS;GACT,mBAAmB,MAAM,iBAAiB,cAAc;GACxD,YAAY;EACb;EACD,MAAM,cAAc,MAAM,kBAAkB,qBAC1C,KAAK,QAAQ,EACb,aAAa,IAAI,kBAAkB,EACnC,eAAe,OACf,QACA,OACA,QACA,QACA,eAAe,QAChB;EAGD,MAAMkB,uBAAiC,CAAE;EACzC,MAAM,UAAU,MAAM,QAAQ,WAC5B,aAAa,IAAI,OAAO,aAAa,UAAU;GAE7C,MAAM,SACJ,cAAc,2BAA2B,YAAY,CAAC,UAAU;GAClE,MAAM,SAAS,MAAM,MAAM,OAAO,QAAQ,aAAa;AAEvD,OAAI,UAAU,MACZ,qBAAqB,KAAK,MAAM;AAGlC,UAAO;EACR,EAAC,CACH;EAID,MAAM,gBAAgB,QACnB,IAAI,CAAC,QAAQ,WAAW;GAAE;GAAQ,YAAY,cAAc;EAAQ,GAAE,CACtE,OACC,CAAC,EAAE,QAAQ,KACR,OAAO,WAAW,eAAe,OAAO,SAAS,QAClD,OAAO,WAAW,WACrB;EAGH,MAAM,gBAAgB,cAAc,iBAAiB,KAAK;EAC1D,MAAMC,cAA8B,CAAE;EACtC,MAAM,QAAQ,IACZ,cAAc,IAAI,OAAO,EAAE,QAAQ,eAAe,YAAY,EAAE,MAAM;AACpE,OAAI,cAAc,WAAW,aAAa;IACxC,MAAM,SAAS,cAAc;IAC7B,YAAY,KAAK,OAAO,IAAI,CAACC,aAAW;AACtC,SACE,aAAaA,YACbC,2BAAcD,SAAO,QAAQ,IAC7BE,uBAAYF,SAAO,QAAQ,EAC3B;MACAA,SAAO,QAAQ,iBAAiB;OAC9B,cAAc;OACd,eAAe;OACf,cAAc;MACf;AACD,UAAI,kBAAkB,MACpBA,SAAO,UAAUjB,2CAA2BiB,SAAO,QAAQ;KAE9D;KACDA,SAAO,iBAAiB;MACtB,GAAGA,SAAO;MACV,YAAY,CAAE;KACf;AACD,YAAOA;IACR,EAAC;AACF,QAAI,OAAO,QACT,MAAM,YAAY,kBAAkB,OAAO,GAAG,KAAK;AAErD,WAAO,YAAY,aACjB,EACE,aAAa,CAAC,MAAO,EACtB,GACD,QACA,QACA,QACA,EACE,QAAQ,KACT,EACF;GACF,OAAM;IAEL,MAAM,YAAY,eAChB,cAAc,QACd,QACA,QACA,QACA,EACE,QAAQ,KACT,EACF;AACD,WAAO,QAAQ,OAAO,cAAc,OAAO;GAC5C;EACF,EAAC,CACH;EAED,MAAM,SAAS;GACb;GACA;GACA,oBAAoB;EACrB;EAKD,OAAO,eAAe,QAAQH,yBAAS;GACrC,OAAO,cACH,EAAE,QAAQ,aAAa,IAAI,CAAC,YAAY,QAAQ,MAAM,CAAE,IACxD;GACJ,cAAc;EACf,EAAC;AAEF,SAAO;CACR;;;;;;;;CASD,MAAM,SACJX,UACAiB,SACAC,WACoB;EAEpB,IAAIC;AACJ,MAAI,MAAM,QAAQ,QAAQ,EACxB,gBAAgB,EAAE,MAAM,QAAS;OAEjC,gBAAgB;EAGlB,MAAM,eAAe,SAAS,IAAI,CAAC,gBACjC,YAAY,IAAIf,yCAA2B,CAC5C;EAED,MAAM,CAAC,gBAAgB,YAAY,GACjC,KAAK,6CAA6C,cAAc;EAClE,eAAe,YAAY,eAAe,aAAa;AAEvD,MAAI,CAAC,KAAK,MACR,QAAO,KAAK,kBAAkB,cAAc,aAAa,eAAe;EAG1E,MAAM,EAAE,OAAO,GAAG;EAClB,MAAM,eAAe,KAAK,wCACxB,YACD;EAED,MAAM,EAAE,aAAa,sBAAsB,oBAAoB,GAC7D,MAAM,KAAK,gBAAgB;GACzB,UAAU;GACV;GACA;GACA,eAAe;GACf,gBAAgB;EACjB,EAAC;EAEJ,IAAI,YAAY,CAAE;AAClB,MAAI,qBAAqB,SAAS,GAAG;GACnC,MAAM,UAAU,MAAM,KAAK,kBACzB,qBAAqB,IAAI,CAAC,MAAM,aAAa,GAAG,EAChD,aACA,gBACA,uBAAuB,SACnB,qBAAqB,IAAI,CAAC,MAAM,qBAAqB,GAAG,GACxD,OACL;GACD,MAAM,QAAQ,IACZ,QAAQ,YAAY,IAAI,OAAO,YAAY,UAAU;IACnD,MAAM,cAAc,qBAAqB;IACzC,YAAY,eAAe;IAE3B,MAAM,SAAS,cAAc,2BAC3B,aAAa,aACd,CAAC,UAAU;AACZ,WAAO,MAAM,OAAO,QAAQ,cAAc,WAAW;GACtD,EAAC,CACH;GACD,YAAY,QAAQ,aAAa,CAAE;EACpC;AAED,SAAO;GAAE;GAAa;EAAW;CAClC;;;;CAMD,iBAAiBgB,UAA2C;AAC1D,SAAO,CAAE;CACV;CAED,aAAqB;AACnB,SAAO;CACR;;;;;;;;CAWD,MAAM,eACJC,cACAJ,SACAC,WACoB;EACpB,MAAMI,iBAAkC,aAAa,IAAI,CAAC,gBACxD,YAAY,gBAAgB,CAC7B;AACD,SAAO,KAAK,SAAS,gBAAgB,SAAS,UAAU;CACzD;CAoDD,qBAIEC,cAIAC,QASI;AACJ,MAAI,OAAO,KAAK,cAAc,WAC5B,OAAM,IAAI,MACR,CAAC,qEAAqE,CAAC;AAG3E,MAAI,QAAQ,OACV,OAAM,IAAI,MACR,CAAC,yDAAyD,CAAC;EAI/D,MAAMC,SACJ;EACF,MAAM,OAAO,QAAQ;EACrB,MAAM,cACJC,iCAAqB,OAAO,IAAI;EAClC,MAAM,SAAS,QAAQ;EACvB,MAAM,aAAa,QAAQ;AAC3B,MAAI,WAAW,WACb,OAAM,IAAI,MACR,CAAC,qFAAqF,CAAC;EAI3F,IAAI,eAAe,QAAQ;EAC3B,IAAIC;AACJ,MAAIC,+BAAmB,OAAO,EAC5B,QAAQ,CACN;GACE,MAAM;GACN,UAAU;IACR,MAAM;IACN;IACA,YAAYC,uCAAa,OAAO;GACjC;EACF,CACF;OACI;AACL,OAAI,UAAU,QACZ,eAAe,OAAO;GAExB,QAAQ,CACN;IACE,MAAM;IACN,UAAU;KACR,MAAM;KACN;KACA,YAAY;IACb;GACF,CACF;EACF;EAED,MAAM,MAAM,KAAK,UAAU,MAAM;EACjC,MAAM,eAAeC,8BAAe,KAClC,CAACC,UAAuC;AACtC,OAAI,CAACC,0BAAe,WAAW,MAAM,CACnC,OAAM,IAAI,MAAM;AAElB,OAAI,CAAC,MAAM,cAAc,MAAM,WAAW,WAAW,EACnD,OAAM,IAAI,MAAM;GAElB,MAAM,WAAW,MAAM,WAAW,KAChC,CAAC,OAAO,GAAG,SAAS,aACrB;AACD,OAAI,CAAC,SACH,OAAM,IAAI,MAAM,CAAC,6BAA6B,EAAE,aAAa,CAAC,CAAC;AAEjE,UAAO,SAAS;EACjB,EACF;AAED,MAAI,CAAC,WACH,QAAO,IAAI,KAAK,aAAa,CAAC,WAAW,EACvC,SAAS,mBACV,EAAC;EAGJ,MAAM,eAAeC,wCAAoB,OAAO,EAE9C,QAAQ,CAACC,OAAYC,aAAW,aAAa,OAAO,MAAM,KAAKA,SAAO,CACvE,EAAC;EACF,MAAM,aAAaF,wCAAoB,OAAO,EAC5C,QAAQ,MAAM,KACf,EAAC;EACF,MAAM,qBAAqB,aAAa,cAAc,EACpD,WAAW,CAAC,UAAW,EACxB,EAAC;AACF,SAAOG,gCAAiB,KAGtB,CACA,EACE,KAAK,IACN,GACD,kBACD,EAAC,CAAC,WAAW,EACZ,SAAS,2BACV,EAAC;CACH;AACF;;;;;AAMD,IAAsB,kBAAtB,cAEU,cAA2B;CAOnC,MAAM,UACJzD,UACAoB,SACAsC,YACqB;EACrB,MAAM,OAAO,MAAM,KAAK,MAAM,UAAU,SAAS,WAAW;EAC5D,MAAM,UAAU,IAAIC,qBAAU;AAC9B,MAAI,OAAO,QAAQ,YAAY,SAC7B,OAAM,IAAI,MACR;AAGJ,SAAO,EACL,aAAa,CACX;GACE,MAAM,QAAQ;GACd;EACD,CACF,EACF;CACF;AACF"}