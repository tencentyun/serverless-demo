{"version":3,"file":"types.d.ts","names":["OpenAI","OpenAIClient","ClientOptions","ResponseFormatText","ResponseFormatJSONObject","ResponseFormatJSONSchema","ResponseFormatTextPython","ResponseFormatTextGrammar","TiktokenModel","BaseLanguageModelCallOptions","InteropZodObject","OpenAIChatModelId","ChatModel","NonNullable","OpenAIVerbosityParam","OpenAICacheRetentionParam","OpenAIApiKey","OpenAIBaseInput","Record","OpenAICoreRequestOptions","RequestOptions","OpenAICallOptions","OpenAIInput","OpenAIChatInput","Chat","ChatCompletionMessageParam","ChatCompletionModality","Array","ChatCompletionAudioParam","Reasoning","Responses","ResponseCreateParams","AzureOpenAIInput","Promise","AzureOpenAIChatInput","ChatOpenAIResponseFormatJSONSchema","Omit","ChatOpenAIReasoningSummary","ResponseReasoningItem","Summary","ChatOpenAIResponseFormat","ResponseFormatConfiguration"],"sources":["../src/types.d.ts"],"sourcesContent":["import type { OpenAI as OpenAIClient, ClientOptions } from \"openai\";\nimport type { ResponseFormatText, ResponseFormatJSONObject, ResponseFormatJSONSchema, ResponseFormatTextPython, ResponseFormatTextGrammar } from \"openai/resources/shared\";\nimport { TiktokenModel } from \"js-tiktoken/lite\";\nimport type { BaseLanguageModelCallOptions } from \"@langchain/core/language_models/base\";\nimport { InteropZodObject } from \"@langchain/core/utils/types\";\nexport type { TiktokenModel };\n/**\n * @see https://platform.openai.com/docs/models\n */\nexport type OpenAIChatModelId = OpenAIClient.ChatModel | (string & NonNullable<unknown>);\nexport type OpenAIVerbosityParam = \"low\" | \"medium\" | \"high\" | null;\nexport type OpenAICacheRetentionParam = \"in-memory\" | \"24h\" | null;\nexport type OpenAIApiKey = ClientOptions[\"apiKey\"];\nexport declare interface OpenAIBaseInput {\n    /** Sampling temperature to use */\n    temperature: number;\n    /**\n     * Maximum number of tokens to generate in the completion. -1 returns as many\n     * tokens as possible given the prompt and the model's maximum context size.\n     */\n    maxTokens?: number;\n    /**\n     * Maximum number of tokens to generate in the completion. -1 returns as many\n     * tokens as possible given the prompt and the model's maximum context size.\n     * Alias for `maxTokens` for reasoning models.\n     */\n    maxCompletionTokens?: number;\n    /** Total probability mass of tokens to consider at each step */\n    topP: number;\n    /** Penalizes repeated tokens according to frequency */\n    frequencyPenalty: number;\n    /** Penalizes repeated tokens */\n    presencePenalty: number;\n    /** Number of completions to generate for each prompt */\n    n: number;\n    /** Dictionary used to adjust the probability of specific tokens being generated */\n    logitBias?: Record<string, number>;\n    /** Unique string identifier representing your end-user, which can help OpenAI to monitor and detect abuse. */\n    user?: string;\n    /** Whether to stream the results or not. Enabling disables tokenUsage reporting */\n    streaming: boolean;\n    /**\n     * Whether or not to include token usage data in streamed chunks.\n     * @default true\n     */\n    streamUsage?: boolean;\n    /**\n     * Model name to use\n     * Alias for `model`\n     * @deprecated Use \"model\" instead.\n     */\n    modelName: string;\n    /** Model name to use */\n    model: OpenAIChatModelId;\n    /** Holds any additional parameters that are valid to pass to {@link\n     * https://platform.openai.com/docs/api-reference/completions/create |\n     * `openai.createCompletion`} that are not explicitly specified on this class.\n     */\n    modelKwargs?: Record<string, any>;\n    /**\n     * List of stop words to use when generating\n     * Alias for `stopSequences`\n     */\n    stop?: string[];\n    /** List of stop words to use when generating */\n    stopSequences?: string[];\n    /**\n     * Timeout to use when making requests to OpenAI.\n     */\n    timeout?: number;\n    /**\n     * API key to use when making requests to OpenAI. Defaults to the value of\n     * `OPENAI_API_KEY` environment variable.\n     * Alias for `apiKey`\n     */\n    openAIApiKey?: OpenAIApiKey;\n    /**\n     * API key to use when making requests to OpenAI. Defaults to the value of\n     * `OPENAI_API_KEY` environment variable.\n     */\n    apiKey?: OpenAIApiKey;\n    /**\n     * The verbosity of the model's response.\n     */\n    verbosity?: OpenAIVerbosityParam;\n}\nexport type OpenAICoreRequestOptions = OpenAIClient.RequestOptions;\nexport interface OpenAICallOptions extends BaseLanguageModelCallOptions {\n    /**\n     * Additional options to pass to the underlying axios request.\n     */\n    options?: OpenAICoreRequestOptions;\n}\n/**\n * Input to OpenAI class.\n */\nexport declare interface OpenAIInput extends OpenAIBaseInput {\n    /** Generates `bestOf` completions server side and returns the \"best\" */\n    bestOf?: number;\n    /** Batch size to use when passing multiple documents to generate */\n    batchSize: number;\n}\nexport interface OpenAIChatInput extends OpenAIBaseInput {\n    /**\n     * Whether to return log probabilities of the output tokens or not.\n     * If true, returns the log probabilities of each output token returned in the content of message.\n     */\n    logprobs?: boolean;\n    /**\n     * An integer between 0 and 5 specifying the number of most likely tokens to return at each token position,\n     * each with an associated log probability. logprobs must be set to true if this parameter is used.\n     */\n    topLogprobs?: number;\n    /** ChatGPT messages to pass as a prefix to the prompt */\n    prefixMessages?: OpenAIClient.Chat.ChatCompletionMessageParam[];\n    /**\n     * Whether to include the raw OpenAI response in the output message's \"additional_kwargs\" field.\n     * Currently in experimental beta.\n     */\n    __includeRawResponse?: boolean;\n    /**\n     * Whether the model supports the `strict` argument when passing in tools.\n     * If `undefined` the `strict` argument will not be passed to OpenAI.\n     */\n    supportsStrictToolCalling?: boolean;\n    /**\n     * Output types that you would like the model to generate for this request. Most\n     * models are capable of generating text, which is the default:\n     *\n     * `[\"text\"]`\n     *\n     * The `gpt-4o-audio-preview` model can also be used to\n     * [generate audio](https://platform.openai.com/docs/guides/audio). To request that\n     * this model generate both text and audio responses, you can use:\n     *\n     * `[\"text\", \"audio\"]`\n     */\n    modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n    /**\n     * Parameters for audio output. Required when audio output is requested with\n     * `modalities: [\"audio\"]`.\n     * [Learn more](https://platform.openai.com/docs/guides/audio).\n     */\n    audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n    /**\n     * Options for reasoning models.\n     *\n     * Note that some options, like reasoning summaries, are only available when using the responses\n     * API. This option is ignored when not using a reasoning model.\n     */\n    reasoning?: OpenAIClient.Reasoning;\n    /**\n     * Should be set to `true` in tenancies with Zero Data Retention\n     * @see https://platform.openai.com/docs/guides/your-data\n     *\n     * @default false\n     */\n    zdrEnabled?: boolean;\n    /**\n     * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\" or \"priority\".\n     * Specifies the service tier for prioritization and latency optimization.\n     */\n    service_tier?: OpenAIClient.Responses.ResponseCreateParams[\"service_tier\"];\n    /**\n     * Used by OpenAI to cache responses for similar requests to optimize your cache\n     * hit rates. Replaces the `user` field.\n     * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n     */\n    promptCacheKey?: string;\n    /**\n     * Used by OpenAI to set cache retention time\n     */\n    promptCacheRetention?: OpenAICacheRetentionParam;\n}\nexport interface AzureOpenAIInput {\n    /**\n     * API version to use when making requests to Azure OpenAI.\n     */\n    azureOpenAIApiVersion?: string;\n    /**\n     * API key to use when making requests to Azure OpenAI.\n     */\n    azureOpenAIApiKey?: string;\n    /**\n     * Azure OpenAI API instance name to use when making requests to Azure OpenAI.\n     * this is the name of the instance you created in the Azure portal.\n     * e.g. \"my-openai-instance\"\n     * this will be used in the endpoint URL: https://my-openai-instance.openai.azure.com/openai/deployments/{DeploymentName}/\n     */\n    azureOpenAIApiInstanceName?: string;\n    /**\n     * Azure OpenAI API deployment name to use for completions when making requests to Azure OpenAI.\n     * This is the name of the deployment you created in the Azure portal.\n     * e.g. \"my-openai-deployment\"\n     * this will be used in the endpoint URL: https://{InstanceName}.openai.azure.com/openai/deployments/my-openai-deployment/\n     */\n    azureOpenAIApiDeploymentName?: string;\n    /**\n     * Azure OpenAI API deployment name to use for embedding when making requests to Azure OpenAI.\n     * This is the name of the deployment you created in the Azure portal.\n     * This will fallback to azureOpenAIApiDeploymentName if not provided.\n     * e.g. \"my-openai-deployment\"\n     * this will be used in the endpoint URL: https://{InstanceName}.openai.azure.com/openai/deployments/my-openai-deployment/\n     */\n    azureOpenAIApiEmbeddingsDeploymentName?: string;\n    /**\n     * Azure OpenAI API deployment name to use for completions when making requests to Azure OpenAI.\n     * Completions are only available for gpt-3.5-turbo and text-davinci-003 deployments.\n     * This is the name of the deployment you created in the Azure portal.\n     * This will fallback to azureOpenAIApiDeploymentName if not provided.\n     * e.g. \"my-openai-deployment\"\n     * this will be used in the endpoint URL: https://{InstanceName}.openai.azure.com/openai/deployments/my-openai-deployment/\n     */\n    azureOpenAIApiCompletionsDeploymentName?: string;\n    /**\n     * Custom base url for Azure OpenAI API. This is useful in case you have a deployment in another region.\n     * e.g. setting this value to \"https://westeurope.api.cognitive.microsoft.com/openai/deployments\"\n     * will be result in the endpoint URL: https://westeurope.api.cognitive.microsoft.com/openai/deployments/{DeploymentName}/\n     */\n    azureOpenAIBasePath?: string;\n    /**\n     * Custom endpoint for Azure OpenAI API. This is useful in case you have a deployment in another region.\n     * e.g. setting this value to \"https://westeurope.api.cognitive.microsoft.com/\"\n     * will be result in the endpoint URL: https://westeurope.api.cognitive.microsoft.com/openai/deployments/{DeploymentName}/\n     */\n    azureOpenAIEndpoint?: string;\n    /**\n     * A function that returns an access token for Microsoft Entra (formerly known as Azure Active Directory),\n     * which will be invoked on every request.\n     */\n    azureADTokenProvider?: () => Promise<string>;\n}\nexport interface AzureOpenAIChatInput extends OpenAIChatInput, AzureOpenAIInput {\n    openAIApiVersion?: string;\n    openAIBasePath?: string;\n    deploymentName?: string;\n}\ntype ChatOpenAIResponseFormatJSONSchema = Omit<ResponseFormatJSONSchema, \"json_schema\"> & {\n    json_schema: Omit<ResponseFormatJSONSchema[\"json_schema\"], \"schema\"> & {\n        /**\n         * The schema for the response format, described as a JSON Schema object\n         * or a Zod object.\n         */\n        schema: Record<string, any> | InteropZodObject;\n    };\n};\n/**\n * The summary of a model's reasoning step.\n */\nexport type ChatOpenAIReasoningSummary = Omit<OpenAIClient.Responses.ResponseReasoningItem, \"summary\"> & {\n    /**\n     * The summary of the reasoning step. The index field will be populated if the response was\n     * streamed. This allows LangChain to recompose the reasoning summary output correctly when the\n     * AIMessage is used as an input for future generation requests.\n     */\n    summary: Array<OpenAIClient.Responses.ResponseReasoningItem.Summary & {\n        index?: number;\n    }>;\n};\nexport type ChatOpenAIResponseFormat = ResponseFormatText | ResponseFormatJSONObject | ResponseFormatTextGrammar | ResponseFormatTextPython | ChatOpenAIResponseFormatJSONSchema;\nexport type ResponseFormatConfiguration = ResponseFormatText | ResponseFormatJSONObject | ResponseFormatJSONSchema;\n//# sourceMappingURL=types.d.ts.map"],"mappings":";;;;;;;;;AASA;AACA;AACYe,KAFAJ,iBAAAA,GAAoBV,QAAAA,CAAaW,SAER,GAAA,CAAA,MAAA,GAF8BC,WAE9B,CAAA,OAAA,CAAA,CAAA;AACzBG,KAFAF,oBAAAA,GAEeZ,KAAAA,GAAAA,QAAa,GAAA,MAAA,GAAA,IAAA;AACfe,KAFbF,yBAAAA,GAE4B,WAAA,GAAA,KAAA,GAAA,IAAA;AAuBxBG,KAxBJF,YAAAA,GAAed,aAwBXgB,CAAAA,QAAAA,CAAAA;AAiBLP,kBAxCcM,eAAAA,CAwCdN;EAKOO;EAiBCF,WAAAA,EAAAA,MAAAA;EAKNA;;AAIuB;AAEpC;EACiBK,SAAAA,CAAAA,EAAAA,MAAAA;EASQC;AAMzB;;;;EAyCYrB,mBAAkB2B,CAAAA,EAAAA,MAAAA;EAOd3B;EAYGA,IAAAA,EAAa6B,MAAAA;EAULf;EAtEcE,gBAAAA,EAAAA,MAAAA;EAAe;EAwEvCe,eAAAA,EAAAA,MAAgB;EA0DhBE;EAKZC,CAAAA,EAAAA,MAAAA;EAA0C9B;EAAL+B,SAAAA,CAAAA,EAzM1BlB,MAyM0BkB,CAAAA,MAAAA,EAAAA,MAAAA,CAAAA;EACpB/B;EAAL+B,IAAAA,CAAAA,EAAAA,MAAAA;EAKDlB;EAAsBR,SAAAA,EAAAA,OAAAA;EAAgB;AAMtD;;;EAMmBT,WAAa6B,CAAAA,EAAAA,OAAUQ;EAA7BX;AAAK;AAIlB;;;EAAuFpB,SAAAA,EAAAA,MAAAA;EAA4BD;EAA2B6B,KAAAA,EA9MnIxB,iBA8MmIwB;EAAkC;AAChL;;;EAA0F9B,WAAAA,CAAAA,EA1MxEa,MA0MwEb,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA;EAAwB;;;;;;;;;;;;;;;;iBAzL/FW;;;;;WAKNA;;;;cAIGF;;KAEJK,wBAAAA,GAA2BlB,QAAAA,CAAamB;UACnCC,iBAAAA,SAA0BZ;;;;YAI7BU;;;;;kBAKWG,WAAAA,SAAoBL;;;;;;UAM5BM,eAAAA,SAAwBN;;;;;;;;;;;;mBAYpBhB,QAAAA,CAAauB,IAAAA,CAAKC;;;;;;;;;;;;;;;;;;;;;;;eAuBtBE,MAAM1B,QAAAA,CAAauB,IAAAA,CAAKE;;;;;;UAM7BzB,QAAAA,CAAauB,IAAAA,CAAKI;;;;;;;cAOd3B,QAAAA,CAAa4B;;;;;;;;;;;;iBAYV5B,QAAAA,CAAa6B,SAAAA,CAAUC;;;;;;;;;;yBAUfhB;;UAEViB,gBAAAA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;+BAwDgBC;;UAEhBC,oBAAAA,SAA6BX,iBAAiBS;;;;;KAK1DG,kCAAAA,GAAqCC,KAAK/B;eAC9B+B,KAAK/B;;;;;YAKNa,sBAAsBR;;;;;;KAM1B2B,0BAAAA,GAA6BD,KAAKnC,QAAAA,CAAa6B,SAAAA,CAAUQ;;;;;;WAMxDX,MAAM1B,QAAAA,CAAa6B,SAAAA,CAAUQ,qBAAAA,CAAsBC;;;;KAIpDC,wBAAAA,GAA2BrC,qBAAqBC,2BAA2BG,4BAA4BD,2BAA2B6B;KAClIM,2BAAAA,GAA8BtC,qBAAqBC,2BAA2BC"}