{"version":3,"file":"codeInterpreter.js","names":["container: string | CodeInterpreterAutoContainer | undefined","options?: CodeInterpreterOptions"],"sources":["../../src/tools/codeInterpreter.ts"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\nimport type { ServerTool } from \"@langchain/core/tools\";\n\n/**\n * Memory limit options for the Code Interpreter container.\n * Higher tiers offer more RAM and are billed at different rates.\n */\nexport type CodeInterpreterMemoryLimit = \"1g\" | \"4g\" | \"16g\" | \"64g\";\n\n/**\n * Auto container configuration for Code Interpreter.\n * Creates a new container automatically or reuses an active one.\n */\nexport interface CodeInterpreterAutoContainer {\n  /**\n   * Memory limit for the container.\n   * - `\"1g\"` (default): 1 GB RAM\n   * - `\"4g\"`: 4 GB RAM\n   * - `\"16g\"`: 16 GB RAM\n   * - `\"64g\"`: 64 GB RAM\n   */\n  memoryLimit?: CodeInterpreterMemoryLimit;\n  /**\n   * Optional list of uploaded file IDs to make available to the code.\n   * Files in the model input are automatically uploaded, so this is only\n   * needed for additional files.\n   */\n  fileIds?: string[];\n}\n\n/**\n * Options for the Code Interpreter tool.\n */\nexport interface CodeInterpreterOptions {\n  /**\n   * The container configuration for the Code Interpreter.\n   *\n   * Can be either:\n   * - A string container ID for explicit mode (created via `/v1/containers` endpoint)\n   * - An auto configuration object that creates/reuses containers automatically\n   *\n   * If not provided, defaults to auto mode with default settings.\n   */\n  container?: string | CodeInterpreterAutoContainer;\n}\n\n/**\n * OpenAI Code Interpreter tool type for the Responses API.\n */\nexport type CodeInterpreterTool = OpenAIClient.Responses.Tool.CodeInterpreter;\n\n/**\n * Converts container options to the API format.\n */\nfunction convertContainer(\n  container: string | CodeInterpreterAutoContainer | undefined\n):\n  | string\n  | OpenAIClient.Responses.Tool.CodeInterpreter.CodeInterpreterToolAuto {\n  // If a string container ID is provided, use it directly\n  if (typeof container === \"string\") {\n    return container;\n  }\n\n  // Auto mode configuration\n  return {\n    type: \"auto\",\n    file_ids: container?.fileIds,\n    memory_limit: container?.memoryLimit,\n  };\n}\n\n/**\n * Creates a Code Interpreter tool that allows models to write and run Python code\n * in a sandboxed environment to solve complex problems.\n *\n * Use Code Interpreter for:\n * - **Data analysis**: Processing files with diverse data and formatting\n * - **File generation**: Creating files with data and images of graphs\n * - **Iterative coding**: Writing and running code iteratively to solve problems\n * - **Visual intelligence**: Cropping, zooming, rotating, and transforming images\n *\n * The tool runs in a container, which is a fully sandboxed virtual machine.\n * Containers can be created automatically (auto mode) or explicitly via the\n * `/v1/containers` endpoint.\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-code-interpreter | OpenAI Code Interpreter Documentation}\n *\n * @param options - Configuration options for the Code Interpreter tool\n * @returns A Code Interpreter tool definition to be passed to the OpenAI Responses API\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n *\n * const model = new ChatOpenAI({ model: \"gpt-4.1\" });\n *\n * // Basic usage with auto container (default 1GB memory)\n * const response = await model.invoke(\n *   \"Solve the equation 3x + 11 = 14\",\n *   { tools: [tools.codeInterpreter()] }\n * );\n *\n * // With increased memory limit for larger computations\n * const response = await model.invoke(\n *   \"Analyze this large dataset and create visualizations\",\n *   {\n *     tools: [tools.codeInterpreter({\n *       container: { memoryLimit: \"4g\" }\n *     })]\n *   }\n * );\n *\n * // With specific files available to the code\n * const response = await model.invoke(\n *   \"Process the uploaded CSV file\",\n *   {\n *     tools: [tools.codeInterpreter({\n *       container: {\n *         memoryLimit: \"4g\",\n *         fileIds: [\"file-abc123\", \"file-def456\"]\n *       }\n *     })]\n *   }\n * );\n *\n * // Using an explicit container ID (created via /v1/containers)\n * const response = await model.invoke(\n *   \"Continue working with the data\",\n *   {\n *     tools: [tools.codeInterpreter({\n *       container: \"cntr_abc123\"\n *     })]\n *   }\n * );\n * ```\n *\n * @remarks\n * - Containers expire after 20 minutes of inactivity\n * - While called \"Code Interpreter\", the model knows it as the \"python tool\"\n * - For explicit prompting, ask for \"the python tool\" in your prompts\n * - Files in model input are automatically uploaded to the container\n * - Generated files are returned as `container_file_citation` annotations\n */\nexport function codeInterpreter(options?: CodeInterpreterOptions): ServerTool {\n  return {\n    type: \"code_interpreter\",\n    container: convertContainer(options?.container),\n  } satisfies CodeInterpreterTool;\n}\n"],"mappings":";;;;AAsDA,SAAS,iBACPA,WAGsE;AAEtE,KAAI,OAAO,cAAc,SACvB,QAAO;AAIT,QAAO;EACL,MAAM;EACN,UAAU,WAAW;EACrB,cAAc,WAAW;CAC1B;AACF;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA0ED,SAAgB,gBAAgBC,SAA8C;AAC5E,QAAO;EACL,MAAM;EACN,WAAW,iBAAiB,SAAS,UAAU;CAChD;AACF"}